<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Luke Ellis | Projects</title>
    <link rel="stylesheet" href="style.css" />
  </head>

  <body>
    <header>
      <div class="header-text">
        <h1>Luke <span>Ellis</span> <small>Data Professional</small></h1>
      </div>
    </header>

    <nav>
      <a href="#about">About</a>
      <a href="experience.html">Experience</a>
      <a href="skills.html">Skills</a>
      <a href="projects.html">Projects</a>
      <a href="contact.html">Contact Me</a>
    </nav>

    <main>
      <section class="projects-section">
        <h2>Projects</h2>

        <div class="projects-grid">
          <!-- Project 1 -->
          <div class="project-card" onclick="openModal('projectModal')">
            <h3>NFL 2025 Big Data Bowl</h3>
            <p>
              Developed a random forest classifier to predict the probability of
              designed run plays vs. pass plays for the San Francisco 49ers.
            </p>
          </div>

          <!-- Placeholder for future project -->
          <div class="project-card" onclick="openModal('etlModal')">
            <h3>ETL Pipeline</h3>
            <p>
              Built an ETL pipeline that transforms Community Transit’s GTFS
              data into a star schema warehouse that can process 38,000+ daily
              transit events. This supports location-based search functions like
              ‘find nearby bus stops.’
            </p>
          </div>
        </div>
      </section>

      <!-- Model for Project Details -->
      <div id="projectModal" class="modal">
        <div class="modal-content">
          <span class="close" onclick="closeModal('projectModal')"
            >&times;</span
          >

          <h2>NFL 2025 Big Data Bowl</h2>

          <h3>Overview</h3>
          <p>
            This machine learning model was developed to give NFL defenses a
            competitive advantage against the San Francisco 49ers by predicting
            the probability of designed run plays versus pass plays based on
            pre-snap alignment and game situation. The model enables defensive
            coordinators to make informed audibles that could disrupt offensive
            strategies.
          </p>

          <h3>Data Source & Processing</h3>
          <p>
            The dataset was supplied by the NFL, covering weeks 1-6 of the 2023
            season and focusing exclusively on San Francisco's offensive plays.
            After removing QB kneels and compressing data by gameID and playID,
            the final dataset contained 451 plays across seven engineered
            features. This clean, focused dataset provided a robust foundation
            for training the predictive model.
          </p>

          <h3>Target Variable</h3>
          <p>
            The target variable is binary: (1) for designed running plays and
            (0) for pass plays. To emphasize strategic play-calling analysis, QB
            scrambles were excluded from designed runs, ensuring the model
            focused on intentional offensive schemes rather than impromptu
            decisions.
          </p>

          <h3>Model Architecture & Performance</h3>
          <p>
            The random forest classifier was selected for its ability to handle
            the imbalanced dataset (194 run plays vs. 257 pass plays) using
            class weights to prevent bias. The model achieved strong performance
            metrics with 80.9% accuracy and 87.4% ROC AUC. Given the high cost
            of defensive misalignment, the model prioritized precision,
            achieving 81% accuracy for pass plays and 79% for run plays.
          </p>

          <div class="performance-metrics">
            <h4>Classification Results:</h4>
            <pre>
                    precision    recall  f1-score   support
            Pass       0.82      0.84      0.83        77
            Run        0.79      0.76      0.78        59
            </pre>
          </div>

          <h3>Feature Engineering & Selection</h3>
          <p>
            Features were selected based on factors that significantly influence
            offensive play-calling decisions. Each feature underwent appropriate
            preprocessing techniques:
          </p>

          <ul>
            <li>
              <strong>offenseFormation (17.76% SHAP importance)</strong> – Six
              formation types (SHOTGUN, SINGLEBACK, I_FORM, etc.) processed
              through OneHotEncoding after removing null values.
            </li>

            <li>
              <strong>motionSinceLineset (12.24%)</strong> – Boolean indicator
              of player motion after line set, with nulls imputed using KNN and
              binary encoding applied.
            </li>

            <li>
              <strong>receiverAlignment (8.81%)</strong> – Categorical alignment
              patterns (0x0, 1x1, 2x1, etc.) processed through OneHotEncoding.
            </li>

            <li>
              <strong>presnap_score_difference (3.79%)</strong> – Engineered
              feature capturing score-based play-calling tendencies (positive
              when SF leads, negative when trailing), standardized for model
              input.
            </li>

            <li>
              <strong>down_yardsToGo (3.54%)</strong> – Custom feature
              multiplying down number by yards to go, creating an expanded scale
              that penalizes late downs with high yardage needs.
            </li>

            <li>
              <strong>gameClock_seconds (2.04%)</strong> – Game clock converted
              from MM:SS format to seconds and standardized.
            </li>

            <li>
              <strong>shiftSinceLineset (2.00%)</strong> – Boolean indicating
              player shifts greater than 2.5 yards from line set position,
              binary encoded after dropping null values.
            </li>
          </ul>

          <h3>Feature Correlation Analysis</h3>
          <p>
            Correlation analysis revealed clear patterns distinguishing run vs.
            pass tendencies:
          </p>
          <div class="correlation-data">
            <h4>Strong Run Indicators (Positive Correlation):</h4>
            <ul>
              <li>I_FORM formation (0.407)</li>
              <li>2x1 receiver alignment (0.384)</li>
              <li>Positive score difference (0.282)</li>
              <li>SINGLEBACK formation (0.227)</li>
            </ul>

            <h4>Strong Pass Indicators (Negative Correlation):</h4>
            <ul>
              <li>SHOTGUN formation (-0.379)</li>
              <li>High down/yards combinations (-0.349)</li>
              <li>Motion since line set (-0.287)</li>
              <li>EMPTY formation (-0.278)</li>
            </ul>
          </div>

          <h3>Hyperparameter Optimization</h3>
          <p>
            The model underwent iterative hyperparameter tuning using grid
            search, optimizing for ROC AUC and precision. Cross-validation
            monitoring prevented overfitting, achieving a mean CV score of 88.4%
            with low variability (2.6% standard deviation).
          </p>

          <div class="hyperparameters">
            <h4>Optimal Parameters:</h4>
            <ul>
              <li>bootstrap: False</li>
              <li>max_depth: 10</li>
              <li>max_features: 'sqrt'</li>
              <li>min_samples_leaf: 2</li>
              <li>min_samples_split: 25</li>
              <li>n_estimators: 300</li>
            </ul>
          </div>

          <h3>Model Validation & Learning Curves</h3>
          <div class="project-graphs">
            <img src="assets/graph1.jpg" alt="Confusion Matrix" />
            <img src="assets/graph2.jpg" alt="ROC Curve" />
            <img src="assets/graph3.jpg" alt="Learning Curve" />
          </div>
          <p>
            The learning curve analysis shows strong model performance with
            training data fitting well as dataset size increases. A slight gap
            between training and validation scores suggests minor overfitting
            that could benefit from additional training data. Both curves level
            off, indicating the model has reached a performance plateau with
            current data volume.
          </p>

          <h3>Practical Applications & Business Impact</h3>
          <p>
            This model provides defensive coordinators with actionable
            intelligence to anticipate offensive play calls with 80.9% accuracy.
            The high confidence intervals enable real-time defensive adjustments
            that could significantly impact game outcomes, particularly in
            critical down-and-distance situations.
          </p>

          <h3>Limitations & Future Enhancements</h3>
          <p>
            While the model demonstrates strong predictive capability, several
            areas present opportunities for improvement. Weekly retraining would
            be essential to capture evolving team tendencies throughout the
            season, with potential implementation of time-decay weighting to
            emphasize recent games. Future iterations could incorporate player
            personnel packages, injury reports, and specific player tendencies
            (e.g., Christian McCaffrey's presence) to enhance prediction
            accuracy and provide more granular insights for defensive strategy.
          </p>
        </div>
      </div>
      <!-- ETL Pipeline Modal -->
      <div id="etlModal" class="modal">
        <div class="modal-content">
          <span class="close" onclick="closeModal('etlModal')">&times;</span>

          <h2>ETL Pipeline for Transit Data</h2>

          <h3>Overview</h3>
          <p>
            This comprehensive ETL pipeline was designed to modernize transit
            data management by extracting real-time information from GTFS
            (General Transit Feed Specification) APIs and transforming it into a
            structured star schema data warehouse. The system enables transit
            agencies to perform advanced analytics on ridership patterns, route
            performance, and operational efficiency while maintaining data
            integrity and accessibility.
          </p>

          <h3>Data Sources & Extraction</h3>
          <p>
            The pipeline connects to multiple GTFS-RT (Real-Time) and
            GTFS-Static feeds from major transit agencies including King County
            Metro, Sound Transit, and regional bus systems. Data extraction
            occurs every 30 seconds for real-time feeds and daily for static
            schedule updates. The system handles multiple data formats including
            protocol buffers, JSON, and CSV files, with robust error handling
            for API timeouts and malformed data.
          </p>

          <h3>Data Transformation & Validation</h3>
          <p>
            Raw transit data undergoes extensive transformation to ensure
            consistency and quality. Key processes include timezone
            normalization across different transit systems, route ID
            standardization, stop location geocoding validation, and vehicle
            position interpolation for missing GPS coordinates. Data quality
            checks validate stop times, detect duplicate records, and flag
            anomalous patterns like vehicles traveling impossible speeds.
          </p>

          <div class="performance-metrics">
            <h4>Technology Stack:</h4>
            <ul>
              <li>
                <strong>Python 3.9+</strong> - Core processing engine with
                pandas, requests, and schedule libraries
              </li>
              <li>
                <strong>PostgreSQL 14</strong> - Data warehouse with PostGIS
                extension for geospatial operations
              </li>
              <li>
                <strong>Apache Airflow</strong> - Workflow orchestration and
                dependency management
              </li>
              <li>
                <strong>Redis</strong> - Caching layer for API responses and
                intermediate processing
              </li>
              <li>
                <strong>Docker</strong> - Containerized deployment with
                environment isolation
              </li>
              <li>
                <strong>Prometheus & Grafana</strong> - System monitoring and
                alerting
              </li>
            </ul>
          </div>

          <h3>Star Schema Architecture</h3>
          <p>
            The data warehouse implements a dimensional modeling approach
            optimized for analytical queries. The central fact table captures
            trip events with foreign keys to dimension tables for routes, stops,
            vehicles, and time periods. This design enables efficient
            aggregation queries and supports both historical analysis and
            real-time dashboard updates.
          </p>

          <div class="hyperparameters">
            <h4>Schema Design:</h4>
            <pre>
Fact Table: trip_events (2.3M+ records)
├── trip_id, stop_id, route_id, vehicle_id
├── arrival_time, departure_time, delay_seconds
├── passenger_load, stop_sequence
└── weather_condition_id, date_key

Dimensions:
├── dim_routes: route metadata and service patterns
├── dim_stops: stop locations and accessibility features  
├── dim_vehicles: fleet information and capacity
├── dim_time: date/time hierarchies for temporal analysis
└── dim_weather: conditions affecting service performance
            </pre>
          </div>

          <h3>Pipeline Performance & Scalability</h3>
          <p>
            The system processes over 50,000 vehicle position updates hourly
            across 200+ active routes, maintaining sub-second latency for
            real-time queries. Incremental loading strategies reduce processing
            time by 75% compared to full refreshes, while automated data
            partitioning by month ensures consistent query performance as
            historical data grows.
          </p>

          <div class="correlation-data">
            <h4>Key Performance Metrics:</h4>
            <ul>
              <li>
                Data freshness: 95% of records processed within 60 seconds of
                API availability
              </li>
              <li>
                System uptime: 99.7% availability with automated failover
                mechanisms
              </li>
              <li>
                Query performance: Complex analytical queries execute in under 3
                seconds
              </li>
              <li>
                Storage efficiency: 40% reduction in storage requirements
                through optimal indexing
              </li>
            </ul>
          </div>

          <h3>Data Quality & Monitoring</h3>
          <p>
            Comprehensive data quality frameworks ensure reliable analytics
            output. Automated validation rules check for temporal consistency,
            geographic boundaries, and business logic violations. Real-time
            monitoring tracks data pipeline health, API response times, and data
            volume anomalies, with alert systems notifying administrators of
            critical issues within minutes.
          </p>

          <h3>Business Intelligence Integration</h3>
          <p>
            The warehouse powers multiple analytical applications including
            route optimization dashboards, ridership forecasting models, and
            operational performance reports. Integration with Tableau and Power
            BI enables self-service analytics for transit planners, while
            RESTful APIs provide programmatic access for custom applications and
            mobile transit apps.
          </p>

          <h3>Security & Compliance</h3>
          <p>
            The pipeline implements enterprise-grade security measures including
            encrypted data transmission, role-based access controls, and audit
            logging for all data modifications. GDPR compliance mechanisms
            ensure passenger privacy protection, while automated backup systems
            maintain data availability with 99.9% recovery guarantees.
          </p>

          <h3>Future Enhancements & Roadmap</h3>
          <p>
            Planned improvements include machine learning integration for
            predictive maintenance, expansion to additional transit agencies,
            and real-time passenger counting through mobile app integration.
            Advanced analytics capabilities will incorporate weather data
            correlation, special event impact analysis, and dynamic route
            optimization based on real-time demand patterns.
          </p>
        </div>
      </div>
    </main>

    <footer>
      <p>&copy; 2025 Luke Ellis</p>
    </footer>

    <script>
      function openModal(id) {
        document.getElementById(id).style.display = "block";
      }

      function closeModal(id) {
        document.getElementById(id).style.display = "none";
      }

      window.onclick = function (event) {
        const modals = document.querySelectorAll(".modal");
        modals.forEach((modal) => {
          if (event.target === modal) {
            modal.style.display = "none";
          }
        });
      };
    </script>
  </body>
</html>
